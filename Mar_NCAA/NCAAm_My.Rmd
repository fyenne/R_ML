---
title: "NCAAm_my"
author: "Siming Yan"
date: "3/8/2021"
output: html_document
---

```{r}
library(caret)
```

```{r}
 
train_data_my  = data_matrix[, features]
train_label_my = data_matrix$ResultDiff



train_label_my = train_label_my %>% data.frame()
names(train_label_my) = "train_label_my"
train_label_my = train_label_my %>% mutate(train_label_my =ifelse(train_label_my > 0, "win", "loss"))
# train_label_my %>% table
train_label_my  = train_label_my$train_label_my %>% as.factor()

# train_label_my
```

 
```{r}
trControl = trainControl(
    method = 'cv',
    number = 10,
    # summaryFunction = giniSummary,
    classProbs = TRUE,
    verboseIter = TRUE,
    allowParallel = TRUE)

tuneGridXGB <- expand.grid(
    nrounds = c(475, 500, 525, 550),
    max_depth = c(3, 4, 5),
    eta = c(0.04, 0.05, 0.06),
    gamma = c(5, 10),
    colsample_bytree = c(0.75, .8, .6),
    subsample = c(0.750, .8),
    min_child_weight = c(10, 25, 40))
```

```{r}
library(doParallel)
cl <- makePSOCKcluster(8)
registerDoParallel(cl)
# stopCluster(cl)
# save.image("./based_tune_caret.RData")
```

```{r}
start <- Sys.time()

# train the xgboost learner
xgbmod <- train(
    x = train_data_my,
    y = train_label_my,
    method = 'xgbTree',
    metric = "Accuracy",
    # metric = 'NormalizedGini',
    trControl = trControl,
    tuneGrid = tuneGridXGB,
    verbose = T)


print(Sys.time() - start)
xgbmod$bestTune #500	4	0.05	10	0.9	40	0.75

# 500	3	0.05	5	0.8	25	0.8


```

```{r}
# smooth.spline(x = xgbmod$pred, y = ifelse(data_matrix$ResultDiff > 0, 1, 0))
```

```{r}
sub$Season = as.numeric(substring(sub$ID,1,4))
sub$T1 = as.numeric(substring(sub$ID,6,9))
sub$T2 = as.numeric(substring(sub$ID,11,14))

Z = sub %>% 
  left_join(season_summary_X1, by = c("Season", "T1")) %>% 
  left_join(season_summary_X2, by = c("Season", "T2")) %>%
  left_join(select(seeds, Season, T1 = TeamID, Seed1 = Seed), by = c("Season", "T1")) %>% 
  left_join(select(seeds, Season, T2 = TeamID, Seed2 = Seed), by = c("Season", "T2")) %>% 
  mutate(SeedDiff = Seed1 - Seed2) %>%
  left_join(select(quality, Season, T1 = Team_Id, quality_march_T1 = quality), by = c("Season", "T1")) %>%
  left_join(select(quality, Season, T2 = Team_Id, quality_march_T2 = quality), by = c("Season", "T2"))
```

```{r}
test_my = Z[, features]
# names(test_my)[!names(test_my) %in% names(train_data_my)]
# names(train_data_my)[!names(train_data_my) %in% names(test_my)]
test_my = test_my[, names(train_data_my)]
# names(test_my)
# names(train_data_my)
```


```{r}
preds <- predict(xgbmod, newdata = test_my, type = "prob")
preds
sub$Pred = preds$win
sub = sub[,c(1,3)]
sub
write.csv(sub, "./sub_m_2.csv")

# .0.49459 no medal

```

## caret tune 2

```{r}
# input_x <- as.matrix(select(tr_treated, -SalePrice_clean))
# input_y <- tr_treated$SalePrice_clean
    
# x = train_data_my
# y = train_label_my
input_x <- as.matrix(caret_tune_dada)
input_y <- y_train
```


```{r}
# run with default paras 
grid_default <- expand.grid(
  nrounds = 10,
  max_depth = 6,
  eta = 0.3,
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

train_control <- caret::trainControl(
  method = "none",
  verboseIter = F, # no training log
  allowParallel = TRUE # FALSE for reproducible results 
)

xgb_base <- caret::train(
  x = input_x,
  y = input_y,
  trControl = train_control,
  tuneGrid = grid_default,
  method = "xgbTree",
  verbose = TRUE
)
 
```

#--------------------------------------------
# Grid Search

## 1. Number of Iterations and the Learning Rate

```{r}
nrounds = 200
# note to start nrounds from 200, as smaller learning rates result in errors so
# big with lower starting points that they'll mess the scales
tune_grid <- expand.grid(
  nrounds = seq(from = 100, to = nrounds, by = 50),
  eta = c(0.025, 0.05, 0.1, 0.3),
  max_depth = c(4, 5, 6),
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

tune_control <- caret::trainControl(
  method = "cv", # cross-validation
  number = 3, # with n folds 
  #index = createFolds(tr_treated$Id_clean), # fix the folds
  verboseIter = FALSE, # no training log
  allowParallel = TRUE # FALSE for reproducible results 
)

xgb_tune <- caret::train(
  x = input_x,
  y = input_y,
  trControl = tune_control,
  tuneGrid = tune_grid,
  method = "xgbTree",
  verbose = TRUE
)
```


```{r}
# helper function for the plots
tuneplot <- function(x, probs = .90) {
  ggplot(x) +
    coord_cartesian(ylim = c(quantile(x$results$RMSE, probs = probs), min(x$results$RMSE))) +
    theme_bw()
}

tuneplot(xgb_tune)
```

## 2: Maximum Depth and Minimum Child Weight

```{r}
tune_grid2 <- expand.grid(
  nrounds = seq(from = 50, to = nrounds, by = 50),
  eta = xgb_tune$bestTune$eta,
  max_depth = ifelse(xgb_tune$bestTune$max_depth == 2,
    c(xgb_tune$bestTune$max_depth:4),
    xgb_tune$bestTune$max_depth - 1:xgb_tune$bestTune$max_depth + 1),
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = c(1, 2, 3),
  subsample = 1
)

xgb_tune2 <- caret::train(
  x = input_x,
  y = input_y,
  trControl = tune_control,
  tuneGrid = tune_grid2,
  method = "xgbTree",
  verbose = TRUE
)

tuneplot(xgb_tune2)
```

## 3. Column and Row Sampling
Based on this, we can fix minimum child weight to 3 and maximum depth to 3. Next, weâ€™ll try different values for row and column sampling:

```{r}
tune_grid3 <- expand.grid(
  nrounds = seq(from = 50, to = nrounds, by = 50),
  eta = xgb_tune$bestTune$eta,
  max_depth = xgb_tune2$bestTune$max_depth,
  gamma = 0,
  colsample_bytree = c(0.4, 0.6, 0.8, 1.0),
  min_child_weight = xgb_tune2$bestTune$min_child_weight,
  subsample = c(0.5, 0.75, 1.0)
)

xgb_tune3 <- caret::train(
  x = input_x,
  y = input_y,
  trControl = tune_control,
  tuneGrid = tune_grid3,
  method = "xgbTree",
  verbose = TRUE
)

tuneplot(xgb_tune3, probs = .95)
```

## 4. Gamma

```{r}
tune_grid4 <- expand.grid(
  nrounds = seq(from = 50, to = nrounds, by = 50),
  eta = xgb_tune$bestTune$eta,
  max_depth = xgb_tune2$bestTune$max_depth,
  gamma = c(0, 0.05, 0.1, 0.5, 0.7, 0.9, 1.0),
  colsample_bytree = xgb_tune3$bestTune$colsample_bytree,
  min_child_weight = xgb_tune2$bestTune$min_child_weight,
  subsample = xgb_tune3$bestTune$subsample
)

xgb_tune4 <- caret::train(
  x = input_x,
  y = input_y,
  trControl = tune_control,
  tuneGrid = tune_grid4,
  method = "xgbTree",
  verbose = TRUE
)

tuneplot(xgb_tune4)
```

## 5: Reducing the Learning Rate
Now, we have tuned the hyperparameters and can start reducing the learning rate to get to the final model:

```{r}
tune_grid5 <- expand.grid(
  nrounds = seq(from = 600, to = 10000, by = 100),
  eta = c(0.01, 0.015, 0.025, 0.05, 0.1),
  max_depth = xgb_tune2$bestTune$max_depth,
  gamma = xgb_tune4$bestTune$gamma,
  colsample_bytree = xgb_tune3$bestTune$colsample_bytree,
  min_child_weight = xgb_tune2$bestTune$min_child_weight,
  subsample = xgb_tune3$bestTune$subsample
)

xgb_tune5 <- caret::train(
  x = input_x,
  y = input_y,
  trControl = tune_control,
  tuneGrid = tune_grid5,
  method = "xgbTree",
  verbose = TRUE
)

tuneplot(xgb_tune5)

```

## fit the model with best paras

```{r}
(final_grid <- expand.grid(
  nrounds = xgb_tune5$bestTune$nrounds,
  eta = xgb_tune5$bestTune$eta,
  max_depth = xgb_tune5$bestTune$max_depth,
  gamma = xgb_tune5$bestTune$gamma,
  colsample_bytree = xgb_tune5$bestTune$colsample_bytree,
  min_child_weight = xgb_tune5$bestTune$min_child_weight,
  subsample = xgb_tune5$bestTune$subsample
))
```

```{r}
(xgb_model <- caret::train(
  x = input_x,
  y = input_y,
  trControl = train_control,
  tuneGrid = final_grid,
  method = "xgbTree",
  verbose = TRUE
))
```

